from statistics import mean
import re

import icd10

nlp = None
Linker = None

def build_first_turn(sys_prompt, user_prompt, assistant_response, has_system, is_instruct):
    """
    Generates a chat based on input from the user and assistant response, adding
    roles and content as needed. It returns the compiled chat.

    Args:
        sys_prompt (str): system's response or prompt to be included in the chat
            dialogue.
        user_prompt (str): user's prompt that is to be displayed alongside the
            system's prompt when the function is called.
        assistant_response (str): assistant's response to the user's input, which
            is added to the output chat message.
        has_system (bool): existence of a system role in the given chat, and if
            it is present, it triggers an extension to the chat list with additional
            roles and content.
        is_instruct (bool): ùìéinstruction status of the system, which determines
            whether to include a user prompt and assistant response in the chat output.

    Returns:
        list: a list of chat messages representing a conversation between a user,
        an assistant, and a system.

    """
    if is_instruct:
        if has_system:
            chat = [{"role": "system", "content": sys_prompt}]
            if user_prompt and assistant_response:
                chat.extend([{"role": "user", "content": user_prompt},
                    {"role": "assistant", "content": assistant_response}])
        else:
            if user_prompt and assistant_response:
                chat = [{"role" : "user", "content" : f"{sys_prompt}\n\n{user_prompt}"},
                        {"role": "assistant", "content": assistant_response}]
            else:
                chat = [{"role" : "user", "content" : f"{sys_prompt}"}]
    else:
        chat = f"{sys_prompt}"
        if user_prompt and assistant_response:
            chat += f"\n\n{user_prompt}\n\n{assistant_response}"
    return chat

def build_few_shot_examples(examples, sys_prompt, user_prompt_template, assistant_response_template, has_system, is_instruct):
    """
    Generates a conversation based on given examples, with each example represented
    as a dictionary containing user prompt and assistant response. It creates a
    chat with a mix of user and assistant turns based on the provided instructions.

    Args:
        examples (enumeration.): 5-7 code examples that are given to the AI assistant
            for generating documentation, with each example consisting of a system
            prompt and corresponding user response.
            
            		- `user_prompt`: a string containing a user prompt for the chatbot
            to respond to
            		- `assistant_response`: a string containing an assistant response
            for the chatbot to generate in reply to the user's prompt
            		- `has_system`: a boolean indicating whether the system is supposed
            to have an opinion on the matter being discussed (if true), or not (if
            false)
            		- `is_instruct`: a boolean indicating whether the chatbot should be
            instructed to provide an opinion on the matter being discussed (if
            true), or simply to respond to the user's prompt (if false)
        sys_prompt (str): prompt given by the system, which is used as the first
            message in the conversation.
        user_prompt_template (str): template for the user's prompt, which is
            generated using a dictionary of values representing various pieces of
            information such as the system's name, the assistant's greeting, and
            any additional text to include in the prompt.
        assistant_response_template (str): 2nd half of the response generated by
            the AI assistant, which is created using a template format with variables
            replaced based on the `example` list provided to the function.
        has_system (int): 1st turn of conversation, where it indicates whether it
            is the system's or user's turn in the chatflow.
        is_instruct (bool): instruction mode of the conversation, and when it is
            `True`, the assistant responds with a simple response, while when it
            is `False`, the assistant engages in a back-and-forth conversation
            with the user.

    Returns:
        str: a list of Markdown-formatted chats, one for each input example, with
        the system and user prompts separated by an empty line.

    """
    for i, example in enumerate(examples):
            user_prompt = user_prompt_template.format(**example)
            assistant_response = assistant_response_template.format(**example)
            if i == 0:
                chat = build_first_turn(sys_prompt, user_prompt, assistant_response, has_system, is_instruct)
            else:
                if is_instruct:
                    chat += [{"role": "user", "content": user_prompt},
                            {"role": "assistant", "content": assistant_response}]
                                
                else:
                    chat += f"\n\n{user_prompt}\n\n{assistant_response}"
    return chat

def build_model_input(example, user_prompt_template, model_is_instruct, few_shot_chat, tokenizer):
    """
    Generates high-quality documentation for given code by formatting a list of
    chat messages into a template for input to a machine learning model. It produces
    a prompt for the user and appends it to a list if few_shot_chat is True, then
    returns the resulting string as the model input.

    Args:
        example (ndarray.): code to be documented, which is passed to the tokenizer
            for formatting and outputting as high-quality documentation.
            
            		- `user_prompt`: This is the text to be used as the user prompt for
            the chat generation model. It is a string.
            		- `model_is_instruct`: This is a boolean indicating whether the model
            is being instructed to generate a chat or not. If `True`, then the
            function generates a list of messages for the chat; otherwise, it
            returns a plain text message as input for the model.
            		- `few_shot_chat`: This is an optional list of strings that, if
            provided, will be added to the list of messages generated by the chat
            generation model. Its presence or absence depends on whether the
            `model_is_instruct` argument is `True`.
            
            	In summary, `build_model_input` takes a dictionary-like object
            `example`, and explains its properties related to the user prompt,
            whether the model is instructed to generate a chat, and an optional
            list of messages to be added to the chat generation.
        user_prompt_template (str): user prompt to be generated for the language
            model, which can include specific keywords or phrases based on the
            given example object.
        model_is_instruct (bool): presence or absence of instructions for the
            language model, and affects whether the function adds the input chat
            message to a list or simply appends a fixed prompt to the input.
        few_shot_chat (list): chat inputs to be added to the main conversation
            prompt, when `model_is_instruct` is `True`.
        tokenizer (instance(s) of tokenizer.): tokenized representation of the
            user's message or response, which is applied to the chat template using
            the `apply_chat_template()` function to generate the next prompt for
            the user.
            
            		- `tokenizer`: A pre-trained language model tokenizer object, which
            can be used to generate text inputs for the model.
            		- `apply_chat_template`: A method that takes a list of chat messages
            and modifies them by adding a generation prompt to each message in the
            list.
            		- `add_generation_prompt`: A boolean value that indicates whether
            or not to add a generation prompt to each message in the list.

    Returns:
        str: a sequence of text inputs to be used for training a chatbot model,
        consisting of user prompts and generated responses.

    """
    user_prompt = user_prompt_template.format(**example)
    if model_is_instruct:
        chat = [{"role": "user", "content": f"{user_prompt}"}]
        if few_shot_chat:
            chat = few_shot_chat + chat
        for msg in chat:
            if isinstance(msg, list):
                print(msg)
        model_input = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)

    else:
        model_input = f"\n\n{user_prompt}\n\n"
        if few_shot_chat:
            model_input = few_shot_chat + model_input

    return model_input


def get_UMLS_entities(doc): 
    """
    Loads a pre-trained spaCy model and adds a pipe for entity linking using UMLS
    terminology. It then extracts entities from a given NLTK document and maps
    them to their UMLS equivalents using the linker pipeline.

    Args:
        doc ("PySPACEngine.Document"): text to be processed for entity recognition
            and linking, which is passed through the NLP pipeline consisting of
            Spacy and Scispacy to extract and link entities.
            
            		- `ents`: A list of `Entity` objects containing information about
            entities in the input text. Each `Entity` object has various attributes,
            including `_.kb_ents`, which is a list of tuples containing the KB ID
            and type of entity (e.g., "person", "organization").
            
            	The function then loads the scispacy linker pipeline using `spacy.load()`,
            configures it with the `config` dictionary, and adds it to the nlp
            object. The function also prints a message indicating that the linker
            is being loaded.
            
            	After loading the linker, the function iterates over each entity in
            the input document and checks if it has a KB ID. If it does, it adds
            the entity's KB ID to the `entities` set. Finally, the function returns
            the set of entities.

    Returns:
        set` of canonical names representing linked entities in UMLS: a set of
        canonical entity names obtained from the UMLS linking pipeline.
        
        		- `entities`: A set containing the canonical names of UMLS entities
        extracted from the input document using the scispacy linker.
        
        	The `entities` variable contains a set of unique entity names that have
        been resolved to their corresponding UMLS IDs using the scispacy linker.
        The entities in this set are obtained by iterating through the `ents` list
        in the input document and checking if the `_.kb_ents` property is present,
        indicating that the entity has a linkage to a UMLS entity. If so, the
        canonical name of the entity is added to the `entities` set.
        
        	The `entities` set provides a concise way to access the UMLS entities
        identified in the input document using the scispacy linker. This can be
        useful for downstream NLP tasks that require access to UMLS entities, such
        as information extraction or text classification.

    """
    global nlp, linker
    if nlp is None:
        import spacy
        import scispacy
        from scispacy.linking import EntityLinker
        nlp = spacy.load("en_core_sci_sm")
        nlp.add_pipe("scispacy_linker", config={"resolve_abbreviations": True, "linker_name": "umls"})
        print("Loading scispacy UMLS Linker...")
        linker = nlp.get_pipe("scispacy_linker")

    entities = set()
    for entity in doc.ents:
        if entity._.kb_ents:
            entities.add(linker.kb.cui_to_entity[entity._.kb_ents[0][0]].canonical_name)
    return entities

def compute_UMLS_F1(model_output, label):
    """
    Computes the UMLS F-score between a given document's label and its predicted
    entities using the provided NLP model. It loads a pre-trained spaCy model,
    adds a linking pipeline to resolve abbreviations and link entities to UMLS
    concepts, and then applies the NLP model to both the document and its label.
    The function returns the precision, recall, and F-score of the predicted
    entities in relation to their actual UMLS links.

    Args:
        model_output (str): output of a machine learning model that has been trained
            on medical text data, which is used as input to the UMLS linker pipe
            for entity disambiguation.
        label (nlt.Label.): 14-code labels for the input text, which are used to
            compare with the entities identified by the Entity Linker and determine
            the label assignment precision and F-score.
            
            		- `label`: This is the label data, which is a sequence of strings
            representing the UMLS concepts associated with the text in the input.
            		- `len()`: The length of the `label` sequence.
            		- `[pred for pred in model_output_entities if pred in label_entities]`:
            A list comprehension that retrieves all the UMLS concepts present in
            both the predicted entities and the provided labels.
            		- `[l for l in label_entities if l in model_output_entities]`: Another
            list comprehension that retrieves all the UMLS concepts present in
            both the provided labels and the predicted entities.

    Returns:
        float: a trio of scores (P, R, and F) that measure the similarity between
        UMLS concepts and entities in the input text.

    """
    global nlp, linker
    if nlp is None:
        import spacy
        import scispacy
        from scispacy.linking import EntityLinker
        nlp = spacy.load("en_core_sci_sm")
        nlp.add_pipe("scispacy_linker", config={"resolve_abbreviations": True, "linker_name": "umls"})
        print("Loading scispacy UMLS Linker...")
        linker = nlp.get_pipe("scispacy_linker")
    
    
    doc_model_output = nlp(model_output)
    doc_label = nlp(label)

    model_output_entities = get_UMLS_entities(doc_model_output)
    label_entities = get_UMLS_entities(doc_label)

    if len(model_output_entities) == 0:
        P = 0.0
    else:
        P = len([pred for pred in model_output_entities if pred in label_entities]) / len(model_output_entities)
    if len(label_entities) == 0:
        R = 0.0
    else:
        R = len([l for l in label_entities if l in model_output_entities]) / len(label_entities)

    if (P + R) == 0:
        F = 0.0
    else:
        F = 2 * P * R / (P + R)

    return P, R, F


def update_results(results, new_results):
    """
    Updates a dictionary `results` by appending or replacing existing values with
    new values from a new list `new_results`.

    Args:
        results (list): list that the function modifies by adding new elements or
            overwriting existing ones based on the `new_results` list passed as
            an argument.
        new_results (list): additional output generated by the code for which
            documentation is being created.

    Returns:
        list: a new set of results that includes all values from the `new_results`
        list, as well as any existing values from the `results` dictionary.

    """
    for k in new_results:
        if k not in results:
            results[k] = [new_results[k]]
        results[k].append(new_results[k])
    return None

def compute_average_results(results):
    """
    Takes a list of lists called `results` as input and computes the mean of each
    sublist (i.e., list inside the outer list) and stores the results in a new
    dictionary with the key being the corresponding inner list index and the value
    being the mean of that sublist.

    Args:
        results (list): iterable of numerical values to calculate the mean of.

    Returns:
        dict: a dictionary containing the mean of each value in the `results` list.

    """
    average_results = {}
    for k in results:
        average_results[k] = mean(results[k])
    return average_results


def compute_icd_f1(preds, gt, approximate):
    """
    Computes the ICD F-score by determining the precision and recall of a set of
    predicted labels against a ground truth label set.

    Args:
        preds (set.): 3-letter prefixes of the actual words in the code documentation
            that are likely to be generated by the given function.
            
            		- `preds` is a set of tuples, where each tuple consists of three
            elements - `pred[:3]` represents the first three characters of a
            predicted word, while `gt[:3]` represents the same for the ground truth
            word.
            		- The set of predications `preds` contains all possible predictions
            for the given text, so it is guaranteed to have at least one prediction
            for each ground truth word.
            		- The set of ground truth words `gt` is used to evaluate the accuracy
            of the predictions in `preds`.
            
            	Overall, `compute_icd_f1` returns a tuple of three values:
            
            		- `P`: The proportion of correct predictions among all predicted words.
            		- `R`: The proportion of incorrect predictions among all ground truth
            words.
            		- `F`: The F-score, which is the harmonic mean of P and R.
        gt (set.): 3-element ground truth labels for the given prediction outputs.
            
            		- `gt`: A set of ground truth objects.
            		- `len(gt)`: The number of elements in the `gt` set.
            		- Each element in `gt` is represented by a list `g`, where `g[:3]`
            contains the properties of a single ground truth object (e.g., bounding
            box coordinates, class label).
        approximate (list): 3-character prefix of the predicted and ground truth
            codes to shorten the calculation.

    Returns:
        float: a set of three values: P (precision), R (recall), and F1 score.

    """
    if approximate:
        preds = set([pred[:3] for pred in preds])
        gt = set([g[:3] for g in gt])
    
    if len(preds) == 0:
        P = 0.0
    else:
        P = len([pred for pred in preds if pred in gt]) / len(preds)
    R = len([g for g in gt if g in preds]) / len(gt)

    if (P + R) == 0:
        F = 0.0
    else:
        F = 2 * P * R / (P + R)

    return P, R, F

def is_icd10_valid(code):
    return icd10.find(code) is not None

def parse_icd_codes(text):
    # Regex to match ICD-10 codes
    """
    Uses a regular expression to extract ICD-10 codes from text input. It returns
    a list of matches for any occurrences of valid ICD-10 code patterns in the input.

    Args:
        text (str): input string that is to be searched for matches against the pattern.

    Returns:
        list: a list of matching ICD-10 codes found in the given text.

    """
    pattern = r'\b[A-TV-Z][0-9]{2}(?:\.?[0-9A-Z]{1,4})?\b'

    # Find all matches
    return re.findall(pattern, text)
